{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zvaINt4UpZrE",
        "gVF6yEdKpecE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating YOLO and LLMs for Object Detection in Unity VR\n",
        "\n",
        "## Introduction:\n",
        "\n",
        "In this tutorial, we'll explore the integration of YOLObfor real-time object detection in Unity VR environments. Additionally, we'll delve into LLMS (Large Language Models) to provide transparency and insight into the detection process. By combining these technologies, we'll create immersive VR experiences with powerful object detection capabilities.\n",
        "\n",
        "## Breakdown:\n",
        "\n",
        "1. **Natural Language Processing (NLP):**\n",
        "  NLP is a field of AI focused on enabling computers to understand, interpret, and generate human language. It encompasses tasks like text understanding, language translation, and sentiment analysis. NLP algorithms leverage linguistic principles and machine learning techniques to process and analyze large volumes of text data, enabling applications like virtual assistants and sentiment analysis platforms.\n",
        "\n",
        "2. **LLM (Large Language Models):**\n",
        "  LLMs are advanced AI models, like GPT, proficient in understanding and generating large volumes of natural language text. They revolutionize tasks such as language generation and question answering through extensive pre-training on vast text datasets. Their applications in language understanding and content generation are significant advancements in AI.\n",
        "\n",
        "    **GPT-3.5** is the latest iteration of OpenAI's Generative Pre-trained Transformer (GPT) model, boasting an impressive **175** billion parameters, making it one of the largest language models in existence. This massive scale enables GPT-3.5 to excel across a wide range of natural language tasks, including text completion, translation, question answering, and sentiment analysis. Its ability to generate human-like text and effectively comprehend nuanced prompts has positioned it as a valuable tool across diverse domains, from education and healthcare to content generation.\n",
        "\n",
        "\n",
        "3. **Explainability:**\n",
        "  Explainability in AI refers to the ability of models to provide understandable explanations for their decisions and predictions. It enhances transparency, trust, and accountability in AI systems, particularly in high-stakes domains. Explainability techniques make complex AI models more interpretable by providing insights into their findgs, facilitating human understanding and collaboration.\n",
        "\n",
        "4. **Explainability in AI and Object Detection:**\n",
        "   Explainability techniques like LLMS help users understand why certain objects were detected in a scene and their meaning. This transparency fosters trust and collaboration between users and the AI system, enhancing the overall VR experience.\n",
        "\n",
        "## Conclusion:\n",
        "\n",
        "By leveraging YOLO for object detection and LLMS for explainability with GPT for enhanced interaction, we can create immersive Unity VR experiences with robust object detection capabilities.\n"
      ],
      "metadata": {
        "id": "ax9icP8QlI_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Materials:\n",
        "\n",
        "*   Tutorial #2 Unity Scene\n",
        "  *   Continue working off your Tutorial #2 Unity Scene, this tutorial will add to that scene.\n",
        "\n",
        "**IMPORTANT**\n",
        "\n",
        "\n",
        "---\n",
        "Do **NOT** share this key outside of this course. Access will be turned off at the end of the course, so attempting to share or use it will be **useless**.\n",
        "\n",
        "*   **GPT 3.5 API Key** = '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'\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**IMPORTANT**"
      ],
      "metadata": {
        "id": "Mw4e7LMOnMym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyCharm and Flask"
      ],
      "metadata": {
        "id": "zvaINt4UpZrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to **File** > **New** > and select **Python File**, give it a name of your choice\n",
        "\n",
        "Follow these steps for setting up the Python File"
      ],
      "metadata": {
        "id": "mzSYpi_ugEiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import tensorflow as tf\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import openai\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Assuming these paths are correctly set to your model and labels\n",
        "MODEL_PATH = ''\n",
        "LABELS_PATH = ''\n",
        "\n",
        "# Load the TensorFlow SavedModel\n",
        "model = tf.saved_model.load(MODEL_PATH)\n",
        "print('Model loaded successfully.')\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "\n",
        "def load_labels(label_path):\n",
        "    with open(label_path, 'r') as f:\n",
        "        return [line.strip() for line in f.readlines()]\n",
        "\n",
        "labels = load_labels(LABELS_PATH)\n",
        "\n",
        "def preprocess_image_from_memory(filestr):\n",
        "    npimg = np.frombuffer(filestr, np.uint8)\n",
        "    image = cv2.imdecode(npimg, cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (640, 640))\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "    image = np.transpose(image, (2, 0, 1))\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    return image\n",
        "\n",
        "def detect_with_tensorflow(model, image, labels, confidence_threshold=0.3):\n",
        "    print(\"Detecting with TensorFlow model...\")\n",
        "    inputs = {'input': tf.convert_to_tensor(image)}\n",
        "    outputs = model.signatures['serving_default'](**inputs)\n",
        "    output_tensor = outputs['output'].numpy()\n",
        "\n",
        "    predictions = output_tensor.reshape(-1, 5)\n",
        "\n",
        "    detections = []\n",
        "    for detection in predictions:\n",
        "        x, y, w, h, confidence = detection\n",
        "        if confidence > confidence_threshold:\n",
        "            class_id = 0\n",
        "            detections.append({'confidence': confidence, 'class_id': class_id})\n",
        "\n",
        "    return detections\n",
        "\n",
        "def generate_description(shoe_type, max_tokens=50):\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers in 1 complete sentence.\"},\n",
        "            {\"role\": \"system\", \"content\": \"You are a knowledgeable expert on shoes.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Describe the functionality and typical use cases of {shoe_type}.\"}\n",
        "        ],\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    # Convert the message to a string\n",
        "    message = str(response.choices[0].message)\n",
        "    return message\n",
        "\n",
        "\n",
        "@app.route('/detect', methods=['POST'])\n",
        "def detect():\n",
        "    if 'image' not in request.files:\n",
        "        return jsonify({\"error\": \"No image part in the request\"}), 400\n",
        "\n",
        "    filestr = request.files['image'].read()\n",
        "    image = preprocess_image_from_memory(filestr)\n",
        "    detections = detect_with_tensorflow(model, image, labels, confidence_threshold=0.6)\n",
        "\n",
        "    results = []\n",
        "    for detection in detections:\n",
        "        label = labels[detection['class_id']]\n",
        "        description = generate_description(label)\n",
        "        results.append({\n",
        "            \"label\": label,\n",
        "            \"confidence\": float(detection['confidence']),\n",
        "            \"description\": description\n",
        "        })\n",
        "\n",
        "    return jsonify({\"detections\": results})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True, host='0.0.0.0', port=5000)\n"
      ],
      "metadata": {
        "id": "9IPi5_dypYxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flask Application with TensorFlow Object Detection and GPT-3 Description Generation\n",
        "\n",
        "\n",
        "## Application Setup and Routes\n",
        "\n",
        "The Flask application is initialized and configured to listen for incoming HTTP POST requests on the `/detect` endpoint. This endpoint expects an image file as part of the request, which it processes to detect objects and generate descriptions.\n",
        "\n",
        "```python\n",
        "app = Flask(__name__)\n",
        "```\n",
        "\n",
        "### Environment Configuration\n",
        "\n",
        "The OpenAI API key is set using an environment variable. This key is essential for authenticating requests sent to OpenAI's GPT-3 model.\n",
        "\n",
        "```python\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'your_openai_api_key_here'\n",
        "```\n",
        "\n",
        "## Image Processing and Object Detection\n",
        "\n",
        "Upon receiving an image, the application performs preprocessing to format the image for the TensorFlow model. This includes resizing and normalizing the image data.\n",
        "\n",
        "```python\n",
        "image = preprocess_image_from_memory(filestr)\n",
        "```\n",
        "\n",
        "The TensorFlow model then processes the preprocessed image to detect objects. Each detection is accompanied by a confidence score.\n",
        "\n",
        "```python\n",
        "detections = detect_with_tensorflow(model, image, labels, confidence_threshold=0.6)\n",
        "```\n",
        "\n",
        "## GPT-3 Description Generation\n",
        "\n",
        "For each detected object, the application uses OpenAI's GPT-3 to generate a descriptive text. This is where the focus on the GPT side of things comes in.\n",
        "\n",
        "### Generating Descriptions with GPT-3\n",
        "\n",
        "The `generate_description` function crafts a prompt for GPT-3 based on the detected object. It then sends this prompt to the GPT-3 model and formats the response into a coherent description.\n",
        "\n",
        "```python\n",
        "description = generate_description(label)\n",
        "```\n",
        "\n",
        "This function demonstrates interacting with the OpenAI API, specifying the model to use (`gpt-3.5-turbo` in this case), and how to structure the prompt for effective results.\n",
        "\n",
        "### OpenAI API Integration\n",
        "\n",
        "The integration with OpenAI's API is highlighted in this segment, showcasing how to construct a request with a series of messages designed to guide GPT-3 in generating a relevant and concise description of the detected object.\n",
        "\n",
        "```python\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers in 1 complete sentence.\"},\n",
        "        {\"role\": \"system\", \"content\": \"You are a knowledgeable expert on shoes.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Describe the functionality and typical use cases of {shoe_type}.\"}\n",
        "    ],\n",
        "    max_tokens=max_tokens\n",
        ")\n",
        "```\n",
        "\n",
        "## Response Formatting and Server Response\n",
        "\n",
        "The application aggregates the detection and description data, formatting it into a JSON response that is returned to the client.\n",
        "\n",
        "```python\n",
        "return jsonify({\"detections\": results})\n",
        "```\n",
        "\n",
        "This final step sends a structured response back to the requester, containing both the label and the generated description for each detected object, showcasing a practical application of combining machine learning models for vision and language.\n",
        "\n",
        "## Running the Flask Application\n",
        "\n",
        "```python\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True, host='0.0.0.0', port=5000)\n",
        "```\n",
        "\n",
        "The application is configured to run in debug mode on all network interfaces, making it accessible for testing purposes.\n"
      ],
      "metadata": {
        "id": "AD_RvCE3brR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hit the green **play** button in the top right corner to run the app. In your PyCharm console, you should see that your Flask app is running. Two links will be present. This means your Flask app is successfully launched and ready to accept requests.\n",
        "\n",
        "copy/store the second link, it should look sometwhat like this:\n",
        " **http://192.168.68.116:5000** we will use it in our Unity Code"
      ],
      "metadata": {
        "id": "7sezq3gtfs2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unity"
      ],
      "metadata": {
        "id": "gVF6yEdKpecE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using System;\n",
        "using System.Collections;\n",
        "using System.Collections.Generic;\n",
        "using UnityEngine;\n",
        "using UnityEngine.Networking;\n",
        "using TMPro;\n",
        "\n",
        "[Serializable]\n",
        "public class Detection\n",
        "{\n",
        "    public float confidence;\n",
        "    public string label;\n",
        "    public string description; // Initially holds the complete ChatCompletionMessage string.\n",
        "}\n",
        "\n",
        "[Serializable]\n",
        "public class RootObject\n",
        "{\n",
        "    public List<Detection> detections;\n",
        "}\n",
        "\n",
        "public class ContinuousObjectDetection : MonoBehaviour\n",
        "{\n",
        "    public string serverURL = \"address/detect\";\n",
        "    public int captureWidth = 640;\n",
        "    public int captureHeight = 640;\n",
        "    public float captureIntervalSeconds = 0.1f;\n",
        "    private Camera _camera;\n",
        "    public TextMeshProUGUI detectionResultsText;\n",
        "    public TextMeshProUGUI descriptionResultsText;\n",
        "\n",
        "    private Dictionary<string, string> labelDescriptions = new Dictionary<string, string>();\n",
        "    private HashSet<string> detectedLabelsThisFrame = new HashSet<string>();\n",
        "\n",
        "    void Start()\n",
        "    {\n",
        "        _camera = GetComponent<Camera>();\n",
        "        if (_camera == null)\n",
        "        {\n",
        "            Debug.LogError(\"Camera component not found.\");\n",
        "            return;\n",
        "        }\n",
        "        if (detectionResultsText == null || descriptionResultsText == null)\n",
        "        {\n",
        "            Debug.LogError(\"TextMeshProUGUI component(s) not set.\");\n",
        "            return;\n",
        "        }\n",
        "        Debug.Log(\"Camera found, capture starting.\");\n",
        "        InvokeRepeating(nameof(CaptureAndSend), 2.0f, captureIntervalSeconds);\n",
        "    }\n",
        "\n",
        "    void CaptureAndSend()\n",
        "    {\n",
        "        Debug.Log(\"Preparing to capture and send image.\");\n",
        "        StartCoroutine(CaptureAndSendCoroutine());\n",
        "    }\n",
        "\n",
        "    IEnumerator CaptureAndSendCoroutine()\n",
        "    {\n",
        "        detectedLabelsThisFrame.Clear(); // Clear the set at the start of each detection cycle.\n",
        "        Debug.Log(\"Inside coroutine, capturing frame.\");\n",
        "        yield return new WaitForEndOfFrame();\n",
        "\n",
        "        RenderTexture renderTexture = new RenderTexture(captureWidth, captureHeight, 24);\n",
        "        _camera.targetTexture = renderTexture;\n",
        "        _camera.Render();\n",
        "\n",
        "        Texture2D screenShot = new Texture2D(captureWidth, captureHeight, TextureFormat.RGB24, false);\n",
        "        RenderTexture.active = renderTexture;\n",
        "        screenShot.ReadPixels(new Rect(0, 0, captureWidth, captureHeight), 0, 0);\n",
        "        screenShot.Apply();\n",
        "\n",
        "        byte[] imageData = screenShot.EncodeToJPG();\n",
        "\n",
        "        _camera.targetTexture = null;\n",
        "        RenderTexture.active = null;\n",
        "        Destroy(renderTexture);\n",
        "        Destroy(screenShot);\n",
        "\n",
        "        WWWForm form = new WWWForm();\n",
        "        form.AddBinaryData(\"image\", imageData, \"image.jpg\", \"image/jpeg\");\n",
        "\n",
        "        using (UnityWebRequest www = UnityWebRequest.Post(serverURL, form))\n",
        "        {\n",
        "            yield return www.SendWebRequest();\n",
        "\n",
        "            if (www.result != UnityWebRequest.Result.Success)\n",
        "            {\n",
        "                Debug.LogError($\"Error sending image: {www.error}\");\n",
        "                ClearText(); // Clear text if there's an error.\n",
        "            }\n",
        "            else\n",
        "            {\n",
        "                string jsonResponse = www.downloadHandler.text;\n",
        "                Debug.Log($\"Image uploaded successfully! Response: {jsonResponse}\");\n",
        "                ProcessDetections(jsonResponse);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Clear labels and descriptions if no detections were made this frame.\n",
        "        if (detectedLabelsThisFrame.Count == 0)\n",
        "        {\n",
        "            ClearText();\n",
        "        }\n",
        "    }\n",
        "\n",
        "    void ProcessDetections(string jsonResponse)\n",
        "    {\n",
        "        RootObject rootObject = JsonUtility.FromJson<RootObject>(jsonResponse);\n",
        "        if (rootObject != null && rootObject.detections.Count > 0)\n",
        "        {\n",
        "            foreach (var detection in rootObject.detections)\n",
        "            {\n",
        "                Debug.Log($\"Detected label: {detection.label}, Confidence: {detection.confidence}\");\n",
        "                detectedLabelsThisFrame.Add(detection.label); // Track detected labels this frame.\n",
        "\n",
        "                if (!labelDescriptions.ContainsKey(detection.label))\n",
        "                {\n",
        "                    // Save description only if not already saved.\n",
        "                    labelDescriptions[detection.label] = ExtractDescriptionContent(detection.description);\n",
        "                }\n",
        "\n",
        "                // Update UI with the most recent detection and its description.\n",
        "                detectionResultsText.text = $\"{detection.label} ({detection.confidence * 100:F1}% confidence)\";\n",
        "                descriptionResultsText.text = labelDescriptions[detection.label];\n",
        "            }\n",
        "        }\n",
        "        else\n",
        "        {\n",
        "            Debug.Log(\"No detections in view.\");\n",
        "        }\n",
        "    }\n",
        "\n",
        "    private string ExtractDescriptionContent(string description)\n",
        "    {\n",
        "        const string contentPrefix = \"ChatCompletionMessage(content='\";\n",
        "        int startIdx = description.IndexOf(contentPrefix) + contentPrefix.Length;\n",
        "        if (startIdx >= contentPrefix.Length)\n",
        "        {\n",
        "            int endIdx = description.IndexOf(\"', role='assistant'\", startIdx);\n",
        "            if (endIdx > startIdx)\n",
        "            {\n",
        "                string content = description.Substring(startIdx, endIdx - startIdx);\n",
        "                return content; // Removed line breaks conversion.\n",
        "            }\n",
        "        }\n",
        "        return \"Description not available.\"; // Fallback text if parsing fails or format is unexpected.\n",
        "    }\n",
        "\n",
        "    private void ClearText()\n",
        "    {\n",
        "        detectionResultsText.text = \"\";\n",
        "        descriptionResultsText.text = \"\";\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "6pkxQBFFBIIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuous Object Detection in Unity\n",
        "\n",
        "\n",
        "## Using Directives\n",
        "\n",
        "- `System`: Provides base class and interface functionalities.\n",
        "- `System.Collections`: Contains interfaces and classes that define various collections of objects.\n",
        "- `System.Collections.Generic`: Provides classes for strongly typed collections.\n",
        "- `UnityEngine`: Main namespace for Unity engine functionalities.\n",
        "- `UnityEngine.Networking`: Contains classes for network operations.\n",
        "- `TMPro`: Namespace for TextMesh Pro functionalities, used for advanced text rendering.\n",
        "\n",
        "## Serializable Classes\n",
        "\n",
        "### `Detection` Class\n",
        "\n",
        "- **Purpose**: Represents a single detection result, including confidence, label, and a description.\n",
        "- **Fields**:\n",
        "  - `confidence`: A `float` indicating the detection confidence level.\n",
        "  - `label`: A `string` representing the label of the detected object.\n",
        "  - `description`: A `string` initially holding a complete message string, intended for further parsing.\n",
        "\n",
        "### `RootObject` Class\n",
        "\n",
        "- **Purpose**: Acts as a container for multiple `Detection` objects.\n",
        "- **Fields**:\n",
        "  - `detections`: A `List<Detection>` holding the detection results.\n",
        "\n",
        "## Main MonoBehaviour Class: `ContinuousObjectDetection`\n",
        "\n",
        "### Fields\n",
        "\n",
        "- Public fields for configuration:\n",
        "  - `serverURL`: Server address for sending images.\n",
        "  - `captureWidth`, `captureHeight`: Dimensions for the captured images.\n",
        "  - `captureIntervalSeconds`: Time interval between captures.\n",
        "  - `detectionResultsText`, `descriptionResultsText`: `TextMeshProUGUI` components for displaying detection results and descriptions.\n",
        "- Private fields for internal logic:\n",
        "  - `_camera`: Reference to the Camera component.\n",
        "  - `labelDescriptions`: Dictionary to hold label descriptions.\n",
        "  - `detectedLabelsThisFrame`: HashSet to track labels detected in the current frame.\n",
        "\n",
        "### MonoBehaviour Methods\n",
        "\n",
        "#### `Start()`\n",
        "\n",
        "- **Purpose**: Initializes the script, checks for necessary components, and starts the continuous capture process.\n",
        "- **Key Operations**:\n",
        "  - Checks for the camera component and UI text components, logging errors if not found.\n",
        "  - Uses `InvokeRepeating` to start the `CaptureAndSend` method at a set interval.\n",
        "\n",
        "#### `CaptureAndSend()`\n",
        "\n",
        "- **Purpose**: Prepares for capturing an image and starts the coroutine for the capture and send process.\n",
        "\n",
        "#### `CaptureAndSendCoroutine()`\n",
        "\n",
        "- **Coroutine**: Handles the actual capture, encoding, and sending of the image to the server.\n",
        "- **Process**:\n",
        "  - Clears detected labels from the previous frame.\n",
        "  - Captures the image using a `RenderTexture` and `Texture2D`.\n",
        "  - Encodes the image to JPG format and prepares a `WWWForm`.\n",
        "  - Sends the image to the server using `UnityWebRequest`.\n",
        "  - On success, processes the server response; on failure, clears the UI text.\n",
        "\n",
        "### Utility Methods\n",
        "\n",
        "#### `ProcessDetections(string jsonResponse)`\n",
        "\n",
        "- **Purpose**: Parses the JSON response, updates the UI, and stores the descriptions.\n",
        "- **Key Operations**:\n",
        "  - Parses the JSON response to `RootObject`.\n",
        "  - Updates detected labels and descriptions, handling UI updates.\n",
        "\n",
        "#### `ExtractDescriptionContent(string description)`\n",
        "\n",
        "- **Purpose**: Extracts the actual content from the description string.\n",
        "- **Key Operations**:\n",
        "  - Searches for specific substrings to locate and extract the relevant content.\n",
        "\n",
        "#### `ClearText()`\n",
        "\n",
        "- **Purpose**: Clears the UI text components.\n"
      ],
      "metadata": {
        "id": "BTx5TXCbdqZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ensure you have added another text element for your description\n",
        "  - **Right-click** on the **Canvas** object, then choose **UI > Text - TextMeshPro** to add a text element.\n",
        "  \n",
        "  - click on **Main camera** and drag the text object into the field for **descriptionResultsText** in the Inspector."
      ],
      "metadata": {
        "id": "Qp0tmL7nKFdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positioning for description and label text blocks"
      ],
      "metadata": {
        "id": "O5wAlXCWB7jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's important to achieve the perfect positioning for your text to appear on your VR HUD. Feel free to copy the positioning of this setup or adjust it as you see fit, as long as it remains in front of and visible from your **Main Camera** object."
      ],
      "metadata": {
        "id": "LW2W9bgoCHaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![First Image](https://drive.google.com/uc?export=view&id=1EI0uflu8U4g_LbieaUcSOPVs1i_2bau4)\n",
        "![Second Image](https://drive.google.com/uc?export=view&id=1JfHjR7WyQoa-cNDm7aZmtzrza8NlSPLt)\n"
      ],
      "metadata": {
        "id": "wI1UAKK4BVvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure your Oculus Quest 2 is connected via USB and hit run!\n",
        "\n",
        "**Note**: you might have to get close up with objects for object detection to work."
      ],
      "metadata": {
        "id": "NRcSEdABfIS7"
      }
    }
  ]
}